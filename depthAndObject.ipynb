{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /home/dracero/.local/lib/python3.10/site-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: torch>=1.7 in /home/dracero/.local/lib/python3.10/site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/dracero/.local/lib/python3.10/site-packages (from timm) (0.15.1)\n",
      "Requirement already satisfied: pyyaml in /home/dracero/.local/lib/python3.10/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: safetensors in /home/dracero/.local/lib/python3.10/site-packages (from timm) (0.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (2.14.3)\n",
      "Requirement already satisfied: sympy in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.7.101)\n",
      "Requirement already satisfied: typing-extensions in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (4.6.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (8.5.0.96)\n",
      "Requirement already satisfied: filelock in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/dracero/.local/lib/python3.10/site-packages (from torch>=1.7->timm) (10.9.0.58)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm) (59.6.0)\n",
      "Requirement already satisfied: lit in /home/dracero/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/dracero/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7->timm) (3.26.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dracero/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub->timm) (2.25.1)\n",
      "Requirement already satisfied: fsspec in /home/dracero/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/dracero/.local/lib/python3.10/site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/dracero/.local/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\n",
      "Requirement already satisfied: numpy in /home/dracero/.local/lib/python3.10/site-packages (from torchvision->timm) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dracero/.local/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dracero/.local/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.9.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected person with confidence 0.998 at location [23.88, 40.93, 580.4, 475.56]\n",
      "Detected person with confidence 0.997 at location [23.54, 40.4, 583.61, 475.7]\n",
      "Detected person with confidence 0.999 at location [26.01, 41.51, 583.62, 475.51]\n",
      "Detected person with confidence 0.999 at location [24.36, 41.18, 582.84, 475.42]\n",
      "Detected person with confidence 0.999 at location [25.4, 40.43, 582.7, 475.31]\n",
      "Detected cell phone with confidence 0.965 at location [51.64, 164.86, 168.77, 369.93]\n",
      "Detected person with confidence 0.996 at location [27.19, 41.61, 583.6, 475.6]\n",
      "Detected person with confidence 0.999 at location [21.85, 39.96, 583.7, 475.39]\n",
      "Detected person with confidence 0.998 at location [25.02, 45.05, 582.33, 475.51]\n",
      "Detected person with confidence 0.99 at location [0.26, 50.62, 517.27, 475.18]\n",
      "Detected person with confidence 0.998 at location [0.99, 27.9, 586.26, 474.99]\n",
      "Detected person with confidence 0.992 at location [0.18, 33.38, 613.58, 474.88]\n",
      "Detected person with confidence 0.988 at location [0.09, 1.28, 618.05, 475.07]\n",
      "Detected cell phone with confidence 0.948 at location [377.08, 338.28, 479.45, 477.31]\n",
      "Detected person with confidence 0.996 at location [0.38, 5.48, 639.26, 474.19]\n",
      "Detected cell phone with confidence 0.912 at location [170.58, 68.46, 635.67, 286.22]\n",
      "Detected person with confidence 0.999 at location [-0.05, 32.89, 639.68, 475.01]\n",
      "Detected person with confidence 0.998 at location [0.25, 33.32, 639.4, 474.65]\n",
      "Detected person with confidence 0.999 at location [0.64, 32.03, 638.93, 474.05]\n",
      "Detected person with confidence 0.97 at location [0.86, 33.03, 478.7, 473.86]\n",
      "Detected scissors with confidence 0.941 at location [271.93, 237.53, 415.82, 375.04]\n",
      "Detected person with confidence 0.999 at location [0.04, 34.94, 639.6, 474.67]\n",
      "Detected person with confidence 0.999 at location [-0.1, 32.5, 639.7, 474.53]\n",
      "Detected chair with confidence 0.99 at location [194.4, 1.87, 438.31, 418.35]\n",
      "Detected chair with confidence 0.987 at location [195.69, 160.34, 333.84, 416.29]\n",
      "Detected dog with confidence 0.993 at location [166.87, 1.51, 423.52, 304.42]\n",
      "Detected person with confidence 0.915 at location [-0.65, 1.22, 467.38, 474.86]\n",
      "Detected person with confidence 0.952 at location [94.1, 1.12, 466.61, 474.93]\n",
      "Detected person with confidence 0.916 at location [0.61, 0.95, 618.49, 474.29]\n",
      "Detected dog with confidence 0.916 at location [70.41, 1.24, 555.65, 474.69]\n",
      "Detected dog with confidence 0.975 at location [-0.41, 1.58, 535.87, 475.32]\n",
      "Detected dog with confidence 0.993 at location [0.0, 1.73, 517.39, 475.48]\n",
      "Detected dog with confidence 0.947 at location [-0.12, 1.76, 523.6, 475.82]\n",
      "Detected dog with confidence 0.939 at location [0.67, 1.57, 500.37, 475.26]\n",
      "Detected person with confidence 0.939 at location [300.84, 0.68, 383.71, 132.13]\n",
      "Detected dog with confidence 0.979 at location [1.31, 102.78, 481.87, 475.6]\n",
      "Detected person with confidence 0.96 at location [0.9, 1.13, 470.94, 474.51]\n",
      "Detected person with confidence 0.931 at location [-0.12, 1.66, 343.49, 474.82]\n",
      "Detected couch with confidence 0.962 at location [353.91, 389.76, 639.27, 477.3]\n",
      "Detected person with confidence 1.0 at location [-0.07, 1.55, 639.66, 474.43]\n",
      "Detected cell phone with confidence 0.934 at location [122.35, 158.1, 249.66, 364.79]\n",
      "Detected person with confidence 1.0 at location [0.97, 1.53, 639.56, 474.43]\n",
      "Detected person with confidence 1.0 at location [0.6, 4.53, 639.22, 474.7]\n",
      "Detected person with confidence 0.999 at location [1.75, 6.17, 639.58, 474.65]\n",
      "Detected person with confidence 0.999 at location [3.3, 7.27, 639.54, 474.35]\n",
      "Detected person with confidence 0.998 at location [8.78, 10.8, 640.3, 475.16]\n",
      "Detected person with confidence 0.999 at location [7.82, 9.67, 640.61, 474.99]\n",
      "Detected cell phone with confidence 0.976 at location [126.72, 174.88, 251.76, 381.67]\n",
      "Detected person with confidence 0.999 at location [9.24, 14.36, 639.84, 474.8]\n",
      "Detected cell phone with confidence 0.97 at location [125.05, 175.81, 250.35, 380.99]\n",
      "Detected person with confidence 0.999 at location [11.52, 14.69, 638.45, 474.98]\n",
      "Detected cell phone with confidence 0.967 at location [122.87, 176.75, 249.6, 381.82]\n",
      "Detected person with confidence 0.996 at location [13.78, 15.9, 636.97, 474.72]\n",
      "Detected person with confidence 0.999 at location [11.19, 13.23, 638.08, 474.76]\n",
      "Detected cell phone with confidence 0.983 at location [125.94, 167.56, 250.68, 377.71]\n",
      "Detected person with confidence 0.999 at location [13.46, 13.82, 636.76, 474.88]\n",
      "Detected person with confidence 0.999 at location [12.9, 15.34, 637.57, 474.82]\n",
      "Detected cell phone with confidence 0.971 at location [117.74, 172.94, 246.9, 382.77]\n",
      "Detected person with confidence 0.999 at location [11.27, 16.42, 638.64, 474.56]\n",
      "Detected person with confidence 0.999 at location [13.63, 16.57, 637.26, 474.75]\n",
      "Detected person with confidence 1.0 at location [14.86, 1.37, 636.97, 474.3]\n",
      "Detected person with confidence 0.992 at location [0.4, 0.88, 639.24, 474.73]\n",
      "Detected person with confidence 1.0 at location [0.4, 3.0, 639.19, 473.91]\n",
      "Detected person with confidence 0.999 at location [4.83, 7.01, 639.72, 474.26]\n",
      "Detected person with confidence 1.0 at location [6.01, 11.66, 639.54, 474.47]\n",
      "Detected person with confidence 1.0 at location [4.42, 13.23, 639.52, 474.24]\n",
      "Detected person with confidence 1.0 at location [19.19, 12.02, 638.04, 474.82]\n",
      "Detected cell phone with confidence 0.963 at location [436.61, 373.32, 524.71, 478.47]\n",
      "Detected person with confidence 1.0 at location [7.28, 8.35, 639.68, 474.72]\n",
      "Detected person with confidence 0.999 at location [13.37, 8.22, 636.63, 474.29]\n",
      "Detected person with confidence 0.999 at location [10.33, 7.75, 638.9, 474.42]\n",
      "Detected person with confidence 0.999 at location [7.53, 6.33, 640.41, 474.45]\n",
      "Detected person with confidence 1.0 at location [5.08, 8.1, 639.27, 474.71]\n",
      "Detected person with confidence 1.0 at location [9.26, 7.11, 639.45, 474.42]\n",
      "Detected person with confidence 0.999 at location [13.13, 7.62, 636.69, 474.65]\n",
      "Detected person with confidence 0.999 at location [16.89, 9.31, 637.4, 474.94]\n",
      "Detected person with confidence 0.999 at location [53.05, 8.92, 639.37, 475.39]\n",
      "Detected cell phone with confidence 0.961 at location [247.03, 113.87, 367.43, 327.26]\n",
      "Detected person with confidence 0.999 at location [46.9, 7.94, 639.43, 474.8]\n",
      "Detected person with confidence 0.999 at location [27.18, 5.33, 639.42, 474.82]\n",
      "Detected person with confidence 0.999 at location [26.15, 5.87, 639.37, 474.9]\n",
      "Detected person with confidence 0.999 at location [17.93, 5.26, 637.51, 474.77]\n",
      "Detected person with confidence 0.999 at location [20.49, 3.82, 638.1, 474.15]\n",
      "Detected person with confidence 0.999 at location [28.03, 9.17, 639.59, 475.37]\n",
      "Detected person with confidence 0.999 at location [23.53, 6.71, 638.85, 474.86]\n",
      "Detected person with confidence 0.999 at location [15.8, 3.97, 636.99, 474.22]\n",
      "Detected chair with confidence 0.956 at location [195.31, 1.83, 435.22, 419.46]\n",
      "Detected chair with confidence 0.964 at location [195.45, 155.84, 328.73, 351.72]\n",
      "Detected person with confidence 0.997 at location [125.01, 1.29, 638.84, 474.62]\n",
      "Detected person with confidence 0.998 at location [107.72, 1.91, 638.88, 473.51]\n",
      "Detected cell phone with confidence 0.996 at location [285.28, 89.0, 474.94, 474.59]\n",
      "Detected chair with confidence 0.985 at location [193.42, 98.09, 352.8, 368.13]\n",
      "Detected person with confidence 0.999 at location [132.25, 5.64, 639.52, 474.77]\n",
      "Detected cell phone with confidence 0.955 at location [429.3, 331.46, 528.79, 477.9]\n",
      "Detected person with confidence 0.999 at location [98.2, 1.84, 638.49, 474.31]\n",
      "Detected person with confidence 0.999 at location [93.37, 1.8, 638.62, 474.44]\n",
      "Detected person with confidence 0.999 at location [92.36, 1.77, 638.74, 474.54]\n",
      "Detected cell phone with confidence 0.904 at location [429.87, 333.84, 519.8, 477.45]\n",
      "Detected person with confidence 0.999 at location [97.41, 1.53, 638.35, 474.78]\n",
      "Detected cell phone with confidence 0.923 at location [428.25, 331.4, 522.48, 478.01]\n",
      "Detected person with confidence 0.999 at location [97.3, 1.46, 638.22, 474.69]\n",
      "Detected person with confidence 0.999 at location [96.44, 1.89, 638.17, 474.2]\n",
      "Detected person with confidence 0.999 at location [91.1, 2.99, 638.96, 474.48]\n",
      "Detected cell phone with confidence 0.917 at location [461.71, 330.13, 558.3, 477.81]\n",
      "Detected person with confidence 0.998 at location [119.61, 1.92, 639.22, 474.08]\n",
      "Detected cell phone with confidence 0.946 at location [470.67, 327.77, 566.06, 477.51]\n",
      "Detected person with confidence 0.998 at location [128.8, 2.72, 639.18, 474.07]\n",
      "Detected person with confidence 0.998 at location [112.37, 3.98, 639.23, 474.47]\n",
      "Detected cell phone with confidence 0.931 at location [78.37, 96.97, 306.05, 476.26]\n",
      "Detected person with confidence 0.999 at location [54.04, 3.84, 638.63, 474.33]\n",
      "Detected cell phone with confidence 0.947 at location [60.4, 99.4, 295.45, 476.62]\n",
      "Detected person with confidence 0.999 at location [42.17, 2.87, 639.85, 474.26]\n",
      "Detected cell phone with confidence 0.945 at location [78.24, 105.5, 298.93, 476.27]\n",
      "Detected person with confidence 0.999 at location [48.56, 3.0, 639.25, 474.22]\n",
      "Detected cell phone with confidence 0.935 at location [70.74, 110.68, 288.13, 404.53]\n",
      "Detected person with confidence 0.999 at location [35.18, 1.5, 639.92, 474.02]\n",
      "Detected cell phone with confidence 0.989 at location [85.59, 106.09, 300.08, 475.75]\n",
      "Detected person with confidence 0.999 at location [50.88, 4.48, 639.03, 474.52]\n",
      "Detected cell phone with confidence 0.99 at location [69.09, 107.39, 288.84, 476.14]\n",
      "Detected person with confidence 0.999 at location [37.92, 2.72, 639.96, 474.05]\n",
      "Detected cell phone with confidence 0.915 at location [83.29, 105.95, 294.4, 475.62]\n",
      "Detected person with confidence 0.999 at location [43.54, 3.09, 639.61, 474.3]\n",
      "Detected person with confidence 0.999 at location [46.45, 3.47, 639.38, 474.32]\n",
      "Detected person with confidence 0.999 at location [55.15, 3.62, 638.8, 474.24]\n",
      "Detected person with confidence 0.999 at location [60.25, 6.73, 638.62, 474.95]\n",
      "Detected person with confidence 0.997 at location [97.39, 4.66, 638.39, 474.48]\n",
      "Detected person with confidence 0.996 at location [105.06, 5.89, 638.45, 474.38]\n",
      "Detected person with confidence 0.999 at location [70.17, 4.32, 639.36, 474.52]\n",
      "Detected toothbrush with confidence 0.922 at location [97.54, 53.05, 139.22, 444.63]\n",
      "Detected person with confidence 0.999 at location [66.19, 1.63, 638.96, 473.8]\n",
      "Detected toothbrush with confidence 0.936 at location [116.86, 55.68, 154.45, 451.43]\n",
      "Detected person with confidence 0.999 at location [84.09, 1.25, 639.07, 474.04]\n",
      "Detected person with confidence 0.999 at location [95.19, 2.66, 638.36, 474.27]\n",
      "Detected person with confidence 0.999 at location [79.29, 2.8, 639.28, 474.11]\n",
      "Detected toothbrush with confidence 0.953 at location [126.45, 48.42, 171.94, 439.76]\n",
      "Detected person with confidence 0.999 at location [76.55, 1.72, 639.21, 473.89]\n",
      "Detected toothbrush with confidence 0.925 at location [124.49, 51.74, 168.5, 441.31]\n",
      "Detected person with confidence 0.999 at location [75.03, 1.93, 639.19, 473.83]\n",
      "Detected toothbrush with confidence 0.932 at location [121.85, 47.65, 167.13, 438.88]\n",
      "Detected person with confidence 0.999 at location [71.05, 1.55, 639.21, 473.88]\n",
      "Detected toothbrush with confidence 0.987 at location [123.28, 51.03, 169.45, 443.81]\n",
      "Detected person with confidence 0.999 at location [77.23, 1.4, 639.48, 474.43]\n",
      "Detected person with confidence 0.999 at location [49.42, 3.9, 639.56, 474.16]\n",
      "Detected cell phone with confidence 0.972 at location [124.96, 49.49, 175.44, 441.69]\n",
      "Detected person with confidence 0.999 at location [58.5, 2.96, 638.32, 474.09]\n",
      "Detected person with confidence 0.999 at location [63.37, 4.92, 638.9, 474.55]\n",
      "Detected person with confidence 0.999 at location [2.12, 12.98, 639.41, 474.44]\n",
      "Detected person with confidence 0.998 at location [101.9, 12.69, 638.3, 475.08]\n",
      "Detected person with confidence 0.998 at location [97.59, 31.83, 638.34, 475.43]\n",
      "Detected person with confidence 0.998 at location [98.83, 30.89, 638.14, 474.76]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m inputs \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mpil_image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Forward pass with DETR model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Convert outputs (bounding boxes and class logits) to COCO API\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# let's only keep detections with score > 0.9\u001b[39;00m\n\u001b[1;32m     45\u001b[0m target_sizes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([pil_image\u001b[39m.\u001b[39msize[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py:1456\u001b[0m, in \u001b[0;36mDetrForObjectDetection.forward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1453\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1455\u001b[0m \u001b[39m# First, sent images through DETR base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[0;32m-> 1456\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1457\u001b[0m     pixel_values,\n\u001b[1;32m   1458\u001b[0m     pixel_mask\u001b[39m=\u001b[39;49mpixel_mask,\n\u001b[1;32m   1459\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1460\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1461\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1462\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1463\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1464\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1465\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1466\u001b[0m )\n\u001b[1;32m   1468\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1470\u001b[0m \u001b[39m# class logits + predicted bounding boxes\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py:1313\u001b[0m, in \u001b[0;36mDetrModel.forward\u001b[0;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[39m# Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[39m# flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39m# flattened_mask is a Tensor of shape (batch_size, heigth*width)\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1313\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1314\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49mflattened_features,\n\u001b[1;32m   1315\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mflattened_mask,\n\u001b[1;32m   1316\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m   1317\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1318\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1319\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py:990\u001b[0m, in \u001b[0;36mDetrEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, position_embeddings, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m     layer_outputs \u001b[39m=\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     \u001b[39m# we add position_embeddings as extra input to the encoder_layer\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[1;32m    992\u001b[0m         attention_mask,\n\u001b[1;32m    993\u001b[0m         position_embeddings\u001b[39m=\u001b[39;49mposition_embeddings,\n\u001b[1;32m    994\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    995\u001b[0m     )\n\u001b[1;32m    997\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    999\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py:669\u001b[0m, in \u001b[0;36mDetrEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    666\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    668\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(hidden_states))\n\u001b[1;32m    670\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    672\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "# Load DETR model\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Open the camera\n",
    "cv2.namedWindow(\"preview\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#if cap.isOpened(): # try to get the first frame\n",
    "#    rval, frame = cap.read()\n",
    "#else:\n",
    "#    rval = False\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    #cv2.imshow(\"preview\", frame)\n",
    "    ret, frame = cap.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the frame to PIL image\n",
    "    pil_image = Image.fromarray(np.uint8(rgb_frame)).convert('RGB')\n",
    "\n",
    "    # Preprocess the image with DETR processor\n",
    "    inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass with DETR model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Convert outputs (bounding boxes and class logits) to COCO API\n",
    "    # let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([pil_image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(\n",
    "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "            f\"{round(score.item(), 3)} at location {box}\"\n",
    "        )\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
